{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ssd_300_vgg/conv5_3/Relu:0\", shape=(?, 19, 19, 512), dtype=float32)\n",
      "Tensor(\"ssd_300_vgg/pool5/MaxPool:0\", shape=(?, 19, 19, 512), dtype=float32)\n",
      "Tensor(\"ssd_300_vgg/block4_box/L2Normalization/mul:0\", shape=(?, 38, 38, 512), dtype=float32)\n",
      "Tensor(\"Const:0\", shape=(38, 38, 1), dtype=float32)\n",
      "Tensor(\"Const_4:0\", shape=(19, 19, 1), dtype=float32)\n",
      "Tensor(\"Const_8:0\", shape=(10, 10, 1), dtype=float32)\n",
      "Tensor(\"Const_12:0\", shape=(5, 5, 1), dtype=float32)\n",
      "Tensor(\"Const_16:0\", shape=(3, 3, 1), dtype=float32)\n",
      "Tensor(\"Const_20:0\", shape=(1, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SSD net (vgg_based) 300x300\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd_layers import conv2d, max_pool2d, l2norm, dropout, pad2d, ssd_multibox_layer\n",
    "from ssd_anchors import ssd_anchors_all_layers\n",
    "\n",
    "# SSD parameters\n",
    "SSDParams = namedtuple('SSDParameters', ['img_shape',  # the input image size: 300x300\n",
    "                                         'num_classes',  # number of classes: 20+1\n",
    "                                         'no_annotation_label',\n",
    "                                         'feat_layers', # list of names of layer for detection\n",
    "                                         'feat_shapes', # list of feature map sizes of layer for detection\n",
    "                                         'anchor_size_bounds', # the down and upper bounds of anchor sizes\n",
    "                                         'anchor_sizes',   # list of anchor sizes of layer for detection\n",
    "                                         'anchor_ratios',  # list of rations used in layer for detection\n",
    "                                         'anchor_steps',   # list of cell size (pixel size) of layer for detection\n",
    "                                         'anchor_offset',  # the center point offset\n",
    "                                         'normalizations', # list of normalizations of layer for detection\n",
    "                                         'prior_scaling'   #\n",
    "                                         ])\n",
    "class SSD(object):\n",
    "    \"\"\"SSD net 300\"\"\"\n",
    "    def __init__(self, is_training=True):\n",
    "        self.is_training = is_training\n",
    "        self.threshold = 0.5  # class score threshold\n",
    "        self.ssd_params = SSDParams(img_shape=(300, 300),\n",
    "                                    num_classes=21,\n",
    "                                    no_annotation_label=21,\n",
    "                                    feat_layers=[\"block4\", \"block7\", \"block8\", \"block9\", \"block10\", \"block11\"],\n",
    "                                    feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n",
    "                                    anchor_size_bounds=[0.15, 0.90],  # diff from the original paper\n",
    "                                    anchor_sizes=[(21., 45.),\n",
    "                                                  (45., 99.),\n",
    "                                                  (99., 153.),\n",
    "                                                  (153., 207.),\n",
    "                                                  (207., 261.),\n",
    "                                                  (261., 315.)],\n",
    "                                    anchor_ratios=[[2, .5],\n",
    "                                                   [2, .5, 3, 1. / 3],\n",
    "                                                   [2, .5, 3, 1. / 3],\n",
    "                                                   [2, .5, 3, 1. / 3],\n",
    "                                                   [2, .5],\n",
    "                                                   [2, .5]],\n",
    "                                    anchor_steps=[8, 16, 32, 64, 100, 300],\n",
    "                                    anchor_offset=0.5,\n",
    "                                    normalizations=[20, -1, -1, -1, -1, -1],\n",
    "                                    prior_scaling=[0.1, 0.1, 0.2, 0.2]\n",
    "                                    )\n",
    "\n",
    "        predictions, logits, locations = self._built_net()\n",
    "        #self._update_feat_shapes_from_net()\n",
    "        classes, scores, bboxes = self._bboxes_select(predictions, locations)\n",
    "        self._classes = classes\n",
    "        self._scores = scores\n",
    "        self._bboxes = bboxes\n",
    "\n",
    "    def _built_net(self):\n",
    "        \"\"\"Construct the SSD net\"\"\"\n",
    "        self.end_points = {}  # record the detection layers output\n",
    "        self._images = tf.placeholder(tf.float32, shape=[None, self.ssd_params.img_shape[0],\n",
    "                                                        self.ssd_params.img_shape[1], 3])\n",
    "        with tf.variable_scope(\"ssd_300_vgg\"):\n",
    "            # original vgg layers\n",
    "            # block 1\n",
    "            net = conv2d(self._images, 64, 3, scope=\"conv1_1\")\n",
    "            net = conv2d(net, 64, 3, scope=\"conv1_2\")\n",
    "            self.end_points[\"block1\"] = net\n",
    "            net = max_pool2d(net, 2, scope=\"pool1\")\n",
    "            # block 2\n",
    "            net = conv2d(net, 128, 3, scope=\"conv2_1\")\n",
    "            net = conv2d(net, 128, 3, scope=\"conv2_2\")\n",
    "            self.end_points[\"block2\"] = net\n",
    "            net = max_pool2d(net, 2, scope=\"pool2\")\n",
    "            # block 3\n",
    "            net = conv2d(net, 256, 3, scope=\"conv3_1\")\n",
    "            net = conv2d(net, 256, 3, scope=\"conv3_2\")\n",
    "            net = conv2d(net, 256, 3, scope=\"conv3_3\")\n",
    "            self.end_points[\"block3\"] = net\n",
    "            net = max_pool2d(net, 2, scope=\"pool3\")\n",
    "            # block 4\n",
    "            net = conv2d(net, 512, 3, scope=\"conv4_1\")\n",
    "            net = conv2d(net, 512, 3, scope=\"conv4_2\")\n",
    "            net = conv2d(net, 512, 3, scope=\"conv4_3\")\n",
    "            self.end_points[\"block4\"] = net\n",
    "            net = max_pool2d(net, 2, scope=\"pool4\")\n",
    "            # block 5\n",
    "            net = conv2d(net, 512, 3, scope=\"conv5_1\")\n",
    "            net = conv2d(net, 512, 3, scope=\"conv5_2\")\n",
    "            net = conv2d(net, 512, 3, scope=\"conv5_3\")\n",
    "            self.end_points[\"block5\"] = net\n",
    "            print(net)\n",
    "            net = max_pool2d(net, 3, stride=1, scope=\"pool5\")\n",
    "            print(net)\n",
    "\n",
    "            # additional SSD layers\n",
    "            # block 6: use dilate conv\n",
    "            net = conv2d(net, 1024, 3, dilation_rate=6, scope=\"conv6\")\n",
    "            self.end_points[\"block6\"] = net\n",
    "            #net = dropout(net, is_training=self.is_training)\n",
    "            # block 7\n",
    "            net = conv2d(net, 1024, 1, scope=\"conv7\")\n",
    "            self.end_points[\"block7\"] = net\n",
    "            # block 8\n",
    "            net = conv2d(net, 256, 1, scope=\"conv8_1x1\")\n",
    "            net = conv2d(pad2d(net, 1), 512, 3, stride=2, scope=\"conv8_3x3\",\n",
    "                         padding=\"valid\")\n",
    "            self.end_points[\"block8\"] = net\n",
    "            # block 9\n",
    "            net = conv2d(net, 128, 1, scope=\"conv9_1x1\")\n",
    "            net = conv2d(pad2d(net, 1), 256, 3, stride=2, scope=\"conv9_3x3\",\n",
    "                         padding=\"valid\")\n",
    "            self.end_points[\"block9\"] = net\n",
    "            # block 10\n",
    "            net = conv2d(net, 128, 1, scope=\"conv10_1x1\")\n",
    "            net = conv2d(net, 256, 3, scope=\"conv10_3x3\", padding=\"valid\")\n",
    "            self.end_points[\"block10\"] = net\n",
    "            # block 11\n",
    "            net = conv2d(net, 128, 1, scope=\"conv11_1x1\")\n",
    "            net = conv2d(net, 256, 3, scope=\"conv11_3x3\", padding=\"valid\")\n",
    "            self.end_points[\"block11\"] = net\n",
    "\n",
    "            # class and location predictions\n",
    "            predictions = []\n",
    "            logits = []\n",
    "            locations = []\n",
    "            for i, layer in enumerate(self.ssd_params.feat_layers):\n",
    "                cls, loc = ssd_multibox_layer(self.end_points[layer], self.ssd_params.num_classes,\n",
    "                                              self.ssd_params.anchor_sizes[i],\n",
    "                                              self.ssd_params.anchor_ratios[i],\n",
    "                                              self.ssd_params.normalizations[i], scope=layer+\"_box\")\n",
    "                predictions.append(tf.nn.softmax(cls))\n",
    "                logits.append(cls)\n",
    "                locations.append(loc)\n",
    "            return predictions, logits, locations\n",
    "\n",
    "    def _update_feat_shapes_from_net(self, predictions):\n",
    "        \"\"\" Obtain the feature shapes from the prediction layers\"\"\"\n",
    "        new_feat_shapes = []\n",
    "        for l in predictions:\n",
    "            new_feat_shapes.append(l.get_shape().as_list()[1:])\n",
    "        self.ssd_params._replace(feat_shapes=new_feat_shapes)\n",
    "\n",
    "    def anchors(self):\n",
    "        \"\"\"Get sSD anchors\"\"\"\n",
    "        return ssd_anchors_all_layers(self.ssd_params.img_shape,\n",
    "                                      self.ssd_params.feat_shapes,\n",
    "                                      self.ssd_params.anchor_sizes,\n",
    "                                      self.ssd_params.anchor_ratios,\n",
    "                                      self.ssd_params.anchor_steps,\n",
    "                                      self.ssd_params.anchor_offset,\n",
    "                                      np.float32)\n",
    "\n",
    "    def _bboxes_decode_layer(self, feat_locations, anchor_bboxes, prior_scaling):\n",
    "        \"\"\"\n",
    "        Decode the feat location of one layer\n",
    "        params:\n",
    "         feat_locations: 5D Tensor, [batch_size, size, size, n_anchors, 4]\n",
    "         anchor_bboxes: list of Tensors(y, x, w, h)\n",
    "                        shape: [size,size,1], [size, size,1], [n_anchors], [n_anchors]\n",
    "         prior_scaling: list of 4 floats\n",
    "        \"\"\"\n",
    "        yref, xref, href, wref = anchor_bboxes\n",
    "        print(yref)\n",
    "        # Compute center, height and width\n",
    "        cx = feat_locations[:, :, :, :, 0] * wref * prior_scaling[0] + xref\n",
    "        cy = feat_locations[:, :, :, :, 1] * href * prior_scaling[1] + yref\n",
    "        w = wref * tf.exp(feat_locations[:, :, :, :, 2] * prior_scaling[2])\n",
    "        h = href * tf.exp(feat_locations[:, :, :, :, 3] * prior_scaling[3])\n",
    "        # compute boxes coordinates (ymin, xmin, ymax,,xmax)\n",
    "        bboxes = tf.stack([cy - h / 2., cx - w / 2.,\n",
    "                           cy + h / 2., cx + w / 2.], axis=-1)\n",
    "        # shape [batch_size, size, size, n_anchors, 4]\n",
    "        return bboxes\n",
    "\n",
    "    def _bboxes_select_layer(self, feat_predictions, feat_locations, anchor_bboxes,\n",
    "                             prior_scaling):\n",
    "        \"\"\"Select boxes from the feat layer, only for bacth_size=1\"\"\"\n",
    "        n_bboxes = np.product(feat_predictions.get_shape().as_list()[1:-1])\n",
    "        # decode the location\n",
    "        bboxes = self._bboxes_decode_layer(feat_locations, anchor_bboxes, prior_scaling)\n",
    "        bboxes = tf.reshape(bboxes, [n_bboxes, 4])\n",
    "        predictions = tf.reshape(feat_predictions, [n_bboxes, self.ssd_params.num_classes])\n",
    "        # remove the background predictions\n",
    "        sub_predictions = predictions[:, 1:]\n",
    "        # choose the max score class\n",
    "        classes = tf.argmax(sub_predictions, axis=1) + 1  # class labels\n",
    "        scores = tf.reduce_max(sub_predictions, axis=1)   # max_class scores\n",
    "        # Boxes selection: use threshold\n",
    "        filter_mask = scores > self.threshold\n",
    "        classes = tf.boolean_mask(classes, filter_mask)\n",
    "        scores = tf.boolean_mask(scores, filter_mask)\n",
    "        bboxes = tf.boolean_mask(bboxes, filter_mask)\n",
    "        return classes, scores, bboxes\n",
    "\n",
    "    def _bboxes_select(self, predictions, locations):\n",
    "        \"\"\"Select all bboxes predictions, only for bacth_size=1\"\"\"\n",
    "        anchor_bboxes_list = self.anchors()\n",
    "        classes_list = []\n",
    "        scores_list = []\n",
    "        bboxes_list = []\n",
    "        # select bboxes for each feat layer\n",
    "        for n in range(len(predictions)):\n",
    "            anchor_bboxes = list(map(tf.convert_to_tensor, anchor_bboxes_list[n]))\n",
    "            classes, scores, bboxes = self._bboxes_select_layer(predictions[n],\n",
    "                            locations[n], anchor_bboxes, self.ssd_params.prior_scaling)\n",
    "            classes_list.append(classes)\n",
    "            scores_list.append(scores)\n",
    "            bboxes_list.append(bboxes)\n",
    "        # combine all feat layers\n",
    "        classes = tf.concat(classes_list, axis=0)\n",
    "        scores = tf.concat(scores_list, axis=0)\n",
    "        bboxes = tf.concat(bboxes_list, axis=0)\n",
    "        return classes, scores, bboxes\n",
    "\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    def detections(self):\n",
    "        return self._classes, self._scores, self._bboxes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ssd = SSD()\n",
    "    sess = tf.Session()\n",
    "    saver_ = tf.train.Saver()\n",
    "    saver_.restore(sess, \"../ssd_checkpoints/ssd_vgg_300_weights.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
